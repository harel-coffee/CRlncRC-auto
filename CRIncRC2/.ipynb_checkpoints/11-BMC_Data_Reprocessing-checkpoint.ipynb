{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. 9号新数据（与BMC Genomics相同）\n",
    "\n",
    "## 大概流程\n",
    "+ 数据清洗（FillNA）\n",
    "+ 特征方差过滤（特征工程）\n",
    "    + 过滤后的特征一共51维（原始86维，方差阈值取1）\n",
    "+ <font color='red'>New</font> 用贝叶斯优化来调参（取代GridSearchCV）\n",
    "    + 参考 https://github.com/fmfn/BayesianOptimization\n",
    "    + 保留最优参数组合\n",
    "+ Cross Validation with Over-sampling\n",
    "    + Plotting\n",
    "    \n",
    "## 细节-方差过滤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from FuncLoadData import LoadData\n",
    "import numpy as np\n",
    "\n",
    "DataFilePath = 'BMCRawData/Data.csv'\n",
    "PositivePath = 'BMCRawData/Positive.csv'\n",
    "NegativePath = 'BMCRawData/Negative.csv'\n",
    "\n",
    "ShuffleSeed = 442\n",
    "X, y, Valid = LoadData(DataFilePath, PositivePath, NegativePath, ShuffleSeed)\n",
    "\n",
    "X = X.fillna(0)\n",
    "# X = np.asarray(X)\n",
    "\n",
    "Valid = Valid.fillna(0)\n",
    "# Valid = np.asarray(Valid)\n",
    "\n",
    "FeatureName = X.columns.values\n",
    "# print (FeatureName)\n",
    "\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "selector = VarianceThreshold(threshold = 1)\n",
    "selector.fit(X)\n",
    "RemainedFeatureLoc = selector.get_support(indices=True)\n",
    "# np.savetxt(\"VarianceFilteredLoc.txt\", RemainedFeatureLoc, fmt='%d')\n",
    "X = selector.transform(X)\n",
    "Valid = selector.transform(Valid)\n",
    "\n",
    "# %%\n",
    "RemainedFeatureName = FeatureName[RemainedFeatureLoc]\n",
    "import pandas as pd\n",
    "\n",
    "TrainingSet = pd.DataFrame(X, index=None, columns=RemainedFeatureName)\n",
    "TrainingSet.to_csv('BMCProcessedData/TrainingSet.csv', index=None)\n",
    "\n",
    "ValidSet = pd.DataFrame(Valid, index=None, columns=RemainedFeatureName)\n",
    "ValidSet.to_csv('BMCProcessedData/ValidSet.csv', index=None)\n",
    "\n",
    "np.savetxt('BMCPProcessedData/TrainingLabel.txt', y, fmt='%d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 细节-贝叶斯优化\n",
    "\n",
    "这部分需要一个新的package，参考那个reference。 安装方法（命令行）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install bayesian-optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "安装好以后，定义一个function，用来返回一个estimator的cross validation mean score（<font color='red'>xgb_cv</font>）  \n",
    "然后定义一个贝叶斯优化器（<font color='red'>optimize_xgb</font>），定义一个参数的选取范围，求取使上一个function取得最大值的参数。  \n",
    "完整过程如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bayes_opt import BayesianOptimization\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import warnings\n",
    "\n",
    "# Prepare xgb for bayes opt\n",
    "def xgb_cv(n_estimator, max_depth, learning_rate, col_bytree, gamma, subsample, data, targets):\n",
    "    estimator = XGBClassifier(n_estimators=n_estimator, \n",
    "                    max_depth=max_depth, \n",
    "                    learning_rate=learning_rate, \n",
    "                    colsample_bytree=col_bytree, \n",
    "                    gamma=gamma, \n",
    "                    subsample=subsample)\n",
    "    cval = cross_val_score(estimator, data, targets, scoring='roc_auc', cv=10)\n",
    "    return cval.mean()\n",
    "\n",
    "def optimize_xgb(data, targets):\n",
    "    \"\"\"Apply Bayesian Optimization to Xgb parameters.\"\"\"\n",
    "    def xgb_crossval(n_estimators, max_depth, learning_rate, colsample_bytree, gamma, subsample):\n",
    "        return xgb_cv(n_estimator = int(n_estimators), \n",
    "                    max_depth = int(max_depth), \n",
    "                    learning_rate = learning_rate, \n",
    "                    col_bytree = colsample_bytree,\n",
    "                    gamma = gamma, \n",
    "                    subsample = subsample,\n",
    "                    data=data, \n",
    "                    targets=targets)\n",
    "\n",
    "    optimizer = BayesianOptimization(\n",
    "        f=xgb_crossval,\n",
    "        pbounds={'n_estimators': (10, 2000),\n",
    "            'max_depth': (3, 10),\n",
    "            'learning_rate': (0.01, 0.3),\n",
    "            'colsample_bytree': (0.7, 1),\n",
    "            'gamma': (0, 0.05),\n",
    "            'subsample': (0.7, 1)},\n",
    "        random_state=442)\n",
    "#     Iteration for 20 times\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.filterwarnings('ignore')\n",
    "        optimizer.maximize(n_iter=20, acq='ei')\n",
    "\n",
    "    return optimizer\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Load Data\n",
    "    X = pd.read_csv(\"BMCProcessedData/TrainingSet.csv\")\n",
    "    y = np.loadtxt(\"BMCProcessedData/TrainingLabel.txt\", dtype = int)\n",
    "\n",
    "    # Over-sampling\n",
    "    from imblearn.over_sampling import SMOTE\n",
    "    resampler = SMOTE(kind='svm', random_state=442)\n",
    "    X_res, y_res = resampler.fit_resample(X, y)\n",
    "\n",
    "    # Standardizing\n",
    "    X_res = StandardScaler().fit_transform(X_res)\n",
    "\n",
    "    OptRes = optimize_xgb(X_res, y_res)\n",
    "    print(\"Final result:\", OptRes.max)\n",
    "\n",
    "    history_df = pd.DataFrame(OptRes.res)\n",
    "    history_df.to_csv('Porto-AUC-10fold-XGB.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最终结果如下：\n",
    "+ colsample_bytree = 0.94\n",
    "+ gamma = 0.03\n",
    "+ learning_rate = 0.124\n",
    "+ max_depth = 10\n",
    "+ n_estimators = 1998\n",
    "+ subsample=0.718\n",
    "\n",
    "## 细节-带Over-sampling的交叉验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from scipy import interp\n",
    "from sklearn.metrics import roc_curve, auc, average_precision_score, precision_recall_curve\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def GetData():\n",
    "    X = pd.read_csv(\"BMCProcessedData/TrainingSet.csv\")\n",
    "    y = np.loadtxt(\"BMCProcessedData/Traininglabel.txt\", dtype=int)\n",
    "\n",
    "    return np.asarray(X), y\n",
    "\n",
    "def GetFoldResult(data, targets, cv, seed):\n",
    "    Stratified_folder = StratifiedKFold(n_splits=cv, random_state=seed)\n",
    "    FolderRes = Stratified_folder.split(data, targets)\n",
    "\n",
    "    data = StandardScaler().fit_transform(data)\n",
    "\n",
    "    # Get each k-fold results\n",
    "    FoldXtrain = []\n",
    "    FoldXtest = []\n",
    "    Foldytrain = []\n",
    "    Foldytest = []\n",
    "\n",
    "    for train_index, test_index in FolderRes:\n",
    "        # Original result for each fold\n",
    "        X_train = data[train_index, :]\n",
    "        y_train = targets[train_index]\n",
    "\n",
    "        X_test = data[test_index, :]\n",
    "        y_test = targets[test_index]\n",
    "        # For each fold, resample the training one\n",
    "        resampler = SMOTE(kind='svm', random_state=seed)\n",
    "        X_res, y_res = resampler.fit_resample(X_train, y_train)\n",
    "\n",
    "        FoldXtrain.append(X_res)\n",
    "        Foldytrain.append(y_res)\n",
    "        FoldXtest.append(X_test)\n",
    "        Foldytest.append(y_test)\n",
    "\n",
    "    return FoldXtrain, FoldXtest, Foldytrain, Foldytest\n",
    "\n",
    "def GetCVScore(X_trainList, X_testList, y_trainList, y_testList, estimator, cv):\n",
    "    print (\"=======================\\nStart calculating AUC for each fold:\")\n",
    "    tprs = []\n",
    "    aucs = []\n",
    "    mean_fpr = np.linspace(0, 1, 100)\n",
    "    tprlist = []\n",
    "    fprlist = []\n",
    "\n",
    "    precisionlist = []\n",
    "    recalllist = []\n",
    "    pres = []\n",
    "    yreal = []\n",
    "    yprob = []\n",
    "    cv_pr = []\n",
    "\n",
    "    for i in range(cv):\n",
    "        X_train = X_trainList[i]\n",
    "        y_train = y_trainList[i]\n",
    "\n",
    "        X_test = X_testList[i]\n",
    "        y_test = y_testList[i]\n",
    "\n",
    "        clf = estimator\n",
    "        clf.fit(X_train, y_train)\n",
    "\n",
    "        y_pred = clf.predict(X_test)\n",
    "        y_pred_prob = clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "        # ROC \n",
    "        fpr, tpr, _ = roc_curve(y_test, y_pred_prob)\n",
    "        fprlist.append(fpr)\n",
    "        tprlist.append(tpr)\n",
    "        tprs.append(interp(mean_fpr, fpr, tpr))\n",
    "        tprs[-1][0] = 0.0\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        aucs.append(roc_auc)\n",
    "\n",
    "        # PRC\n",
    "        _precision, _recall, _ = precision_recall_curve(y_test, y_pred_prob)\n",
    "        precisionlist.append(_precision)\n",
    "        recalllist.append(_recall)\n",
    "        pres.append(interp(mean_fpr, _recall[::-1], _precision[::-1]))\n",
    "        cv_pr.append(average_precision_score(y_test, y_pred_prob))\n",
    "        yreal.append(y_test)\n",
    "        yprob.append(y_pred_prob)\n",
    "\n",
    "        print (\"Fold {} done\".format(i+1))\n",
    "    yreal = np.concatenate(yreal)\n",
    "    yprob = np.concatenate(yprob)\n",
    "    print (\"=======================\")\n",
    "    return fprlist, tprlist, tprs, aucs, pres, yreal, yprob, cv_pr, precisionlist, recalllist\n",
    "    \n",
    "def PlotROC(fprlist, tprlist, tprs, aucs, cvfold):\n",
    "    print (\"Ploting!\")\n",
    "    mean_fpr = np.linspace(0, 1, 100)\n",
    "    mean_tpr = np.mean(tprs, axis = 0)\n",
    "    mean_auc = auc(mean_fpr, mean_tpr)\n",
    "    std_auc = np.std(aucs)\n",
    "\n",
    "    std_tpr = np.std(tprs, axis=0)\n",
    "    tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
    "    tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
    "\n",
    "    plt.figure(figsize=(8, 7))\n",
    "    # Plot each fold\n",
    "    for i in range(cvfold):\n",
    "        plt.plot(fprlist[i], \n",
    "                tprlist[i], \n",
    "                alpha=0.3, \n",
    "                label = \"ROC fold %d (AUC = %.2f)\"%(i, aucs[i]))\n",
    "    # Plot the chance\n",
    "    plt.plot([0, 1], [0, 1], linestyle='--', color='r', alpha=.8)\n",
    "    # Plot the mean one\n",
    "    plt.plot(mean_fpr, \n",
    "            mean_tpr, \n",
    "            color='b', \n",
    "            label=r'Mean ROC (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc, std_auc), alpha=.8)\n",
    "    # Plot the variances\n",
    "    plt.fill_between(mean_fpr, \n",
    "                    tprs_lower, \n",
    "                    tprs_upper, \n",
    "                    color='grey', \n",
    "                    alpha=.2, label=r'$\\pm$ 1 std. dev.')\n",
    "\n",
    "    plt.xlim([-0.05, 1.05])\n",
    "    plt.ylim([-0.05, 1.05])\n",
    "    plt.xlabel('False Positive Rate', fontsize=16)\n",
    "    plt.ylabel('True Positive Rate', fontsize=16)\n",
    "    # plt.title('ROC for each fold')\n",
    "    plt.legend(bbox_to_anchor=(0., 1.02, 1., .102), loc=3,\n",
    "           ncol=2, mode=\"expand\", borderaxespad=0.)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    return plt\n",
    "\n",
    "\n",
    "def PlotPRC(pres, yreal, yprob, prscores, prelist, reclist, cvfold):\n",
    "    print (\"Ploting!\")\n",
    "    mean_rec = np.linspace(0, 1, 100)\n",
    "    mean_pre = np.mean(pres, axis = 0)\n",
    "    mean_pre2, mean_rec2, _ = precision_recall_curve(yreal, yprob)\n",
    "    mean_score = average_precision_score(yreal, yprob)\n",
    "\n",
    "    std_score = np.std(prscores)\n",
    "\n",
    "    std_pre = np.std(pres, axis=0)\n",
    "    pres_upper = np.minimum(mean_pre + std_pre, 1)\n",
    "    pres_lower = np.maximum(mean_pre - std_pre, 0)\n",
    "\n",
    "    plt.figure(figsize=(8, 7))\n",
    "    # Plot each fold\n",
    "    for i in range(cvfold):\n",
    "        plt.plot(reclist[i], \n",
    "                prelist[i],\n",
    "                alpha=0.3, \n",
    "                label = \"PRC fold %d (AP = %.2f)\"%(i, prscores[i]))\n",
    "    # Plot the mean one\n",
    "    plt.plot(mean_rec2, \n",
    "            mean_pre2, \n",
    "            color='b', \n",
    "            label=r'Mean PRC (AP = %0.2f $\\pm$ %0.2f)' % (mean_score, std_score), alpha=.8)\n",
    "    # Plot the variances\n",
    "    plt.fill_between(mean_rec, \n",
    "                    pres_lower, \n",
    "                    pres_upper, \n",
    "                    color='grey', \n",
    "                    alpha=.2, label=r'$\\pm$ 1 std. dev.')\n",
    "\n",
    "    plt.xlim([-0.05, 1.05])\n",
    "    plt.ylim([-0.05, 1.05])\n",
    "    plt.xlabel('Recall', fontsize=16)\n",
    "    plt.ylabel('Precision', fontsize=16)\n",
    "    # plt.title('ROC for each fold')\n",
    "    plt.legend(bbox_to_anchor=(0., 1.02, 1., .102), loc=3,\n",
    "           ncol=2, mode=\"expand\", borderaxespad=0.)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    return plt\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    cv = 10\n",
    "    # seed = 42435 42435 makes auc 0.84\n",
    "    seed = 42435\n",
    "\n",
    "    X, y = GetData()\n",
    "    X_trainList, X_testList, y_trainList, y_testList = GetFoldResult(X, y, cv, seed)\n",
    "\n",
    "    estimator = XGBClassifier(\n",
    "            objective='binary:logistic', \n",
    "            colsample_bytree = 0.94, \n",
    "            gamma = 0.03, \n",
    "            learning_rate = 0.124, \n",
    "            max_depth = 10, \n",
    "            n_estimators = 1998, \n",
    "            subsample=0.718\n",
    "        )\n",
    "\n",
    "    fprlist, tprlist, tprs, aucs, pres, yreal, yprob, prscores, prelist, reclist = GetCVScore(X_trainList, X_testList, y_trainList, y_testList, estimator, cv)\n",
    "\n",
    "    PlotROC(fprlist, tprlist, tprs, aucs, cv)\n",
    "#     plt.savefig(\"10FoldCV_Resample_ROC.png\",bbox_inches = \"tight\")\n",
    "#     plt.savefig(\"10FoldCV_Resample_ROC.eps\",bbox_inches = \"tight\")\n",
    "\n",
    "    PlotPRC(pres, yreal, yprob, prscores, prelist, reclist, cv)\n",
    "#     plt.savefig(\"10FoldCV_Resample_PRC.png\",bbox_inches = \"tight\")\n",
    "#     plt.savefig(\"10FoldCV_Resample_PRC.eps\",bbox_inches = \"tight\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里附上Plotting的结果：  \n",
    "ROC:  \n",
    "![title](Imgs/10FoldCV_Resample_ROC.png)  \n",
    "PRC:  \n",
    "![title](Imgs/10FoldCV_Resample_PRC.png)  \n",
    "\n",
    "Cross Validation Table:  \n",
    "\n",
    "|         |Precision | Recall | F1-Score | Support |\n",
    "|------------|------|------|------|-----|\n",
    "| macro avg fold 0 | 0.74 | 0.56 | 0.59 | 472 |\n",
    "| macro avg fold 1 | 0.79 | 0.59 | 0.63 | 472 |\n",
    "| macro avg fold 2 | 0.60 | 0.58 | 0.59 | 472 |\n",
    "| macro avg fold 3 | 0.82 | 0.68 | 0.73 | 471 |\n",
    "| macro avg fold 4 | 0.67 | 0.59 | 0.62 | 471 |\n",
    "| macro avg fold 5 | 0.65 | 0.65 | 0.65 | 471 |\n",
    "| macro avg fold 6 | 0.69 | 0.62 | 0.64 | 471 |\n",
    "| macro avg fold 7 | 0.76 | 0.68 | 0.71 | 471 |\n",
    "| macro avg fold 8 | 0.67 | 0.59 | 0.62 | 470 |\n",
    "| macro avg fold 9 | 0.74 | 0.60 | 0.63 | 470 |\n",
    "\n",
    "这里的support是只测试集的样本数量（每一个fold的测试集，1/10，差不多就是470个）。 总体的平均值和AUC/PR的值我没有加上去  \n",
    "使用macro avg是因为我们更关注分类器在少数量类别上的表现。\n",
    "\n",
    "\n",
    "## 下面我要做的\n",
    "\n",
    "我想在特征工程上再做些东西，毕竟我们现在只用了个方差过滤, 而目前的性能差距： 只方差过滤(84%)$\\approx$方差过滤后再特征权重过滤(84%)$\\gg$不做特征工程(82%)\n",
    "\n",
    "我目前的设想是这样的：\n",
    "+ 用某种方式做特征的表示的参考标准， 比如， 用信息粒度（Info）\n",
    "+ 然后用某种相似度方式找出这个标准下相似的特征，想办法合并,得到一个新的人工合成特征，起到一个特征工程/降维的作用\n",
    "+ 大概表示如下：\n",
    "    + Synthetized Feature = Integrate(Similarity(InfoA, InfoB, ..., InfoN))\n",
    "+ 这个需要点时间，估计要年后了\n",
    "\n",
    "另外，单边分类(One Class Classification)，以及各种异常检测的方法都可以用在这里（这里少数类就可以当成异常）"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
